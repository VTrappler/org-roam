:PROPERTIES:
:ID:       a5f5f002-3624-4046-b2b7-ee634c77b4d0
:END:
#+title: 2022-01-17
#+startup: latexpreview
* Data Assimilation

#+begin_src emacs-lisp
(setq org-preview-latex-image-directory "confluence/")
#+end_src

#+RESULTS:
: confluence/

** Introduction and definitions
Data Assimilation is a mathematical topic, which aims at combining different sources of

\begin{equation*}
\dot{x^t} =\mathfrak{M}(x^t, t)
\end{equation*}


Let the state $x$ be a $n$ dimensional real column vector, and $y$ is a $p$ dimensional

\begin{equation*}
\left\{
  \begin{array}{rcl}
\dot{x} &=& F(x, t, w) \\
y &=& H(x, t, v)
  \end{array}
\right.
\end{equation*}


\begin{equation*}
\left\{
  \begin{array}{rcl}
x_{n+1} &=& M_n(x_n, w_n) \\
y_n &=& H_n(x_n, v_n)
  \end{array}
\right.
\end{equation*}

\begin{equation*}
\left\{
  \begin{array}{rcl}
x_{n+1} &=& M_n(x_n) +  w_n \\
y_n &=& H_n(x_n) + v_n
  \end{array}
\right.
\end{equation*}


** Bayes Rule

\begin{equation}
\mathbb{P}\left[A \mid B\right] = \frac{\mathbb{P}\left[B \mid A] \mathbb{P}\left[A\right]}{\mathbb{P}[B]}
\end{equation}

or, using probability density functions:
\begin{align*}
p(x\mid y) &= \frac{p(y\mid x)p(x)}{p(y)}= \frac{p(y\mid x)p(x)}{\int p(y\mid x)p(x)\,\mathrm{d}x} \\ &\propto {p(y\mid x)p(x)}
\end{align*}
$p(x\mid y)$ $p(x)$ $p(y) = \int p(y\mid x)p(x) \,\mathrm{d}x$

$p(y\mid x)$


** zae

\begin{equation*}
p(x_{n+1} \mid y_{1:n}) = \int p(x_{n+1} \mid x_n)p(x_n \mid y_{1:n})\,\mathrm{d} x_n
\end{equation*}

\begin{align*}
p(x_{n+1} \mid y_{1:n+1}) &= p(x_{n+1} \mid y_{1:n}, y_{n+1})\\
&\propto p(y_{n+1} \mid x_{n+1}, y_{1:n})p(x_{n+1} \mid y_{1:n})
\end{align*}

\begin{equation*}
p(x_{n+1} \mid y_{1:{n+1}}) \propto p(y_{n+1} \mid x_{n+1})p(x_{n+1} \mid y_{1:n})
\end{equation*}


\begin{equation*}
-\log p(x_{n} \mid y_{n}) = -\log p(y_n \mid x_n) - \log(x_n) + \text{cst}
\end{equation*}

\begin{equation*}
-\log p(y_n \mid x_n) = \frac{1}{2}\|y_n - M(x_n) \|_R^2
\end{equation*}

\begin{equation*}
y_n \mid x_n \sim \mathcal{N}(H(x_n), R)
\end{equation*}
\begin{equation*}
 \mathcal{N}(x^b, B)
\end{equation*}

\begin{equation*}
J_{3D}(x_n) = \frac{1}{2} \|y_n- H(x_n) \|_R^2 + \frac{1}{2}\|x_{n}- x^b \|_B^2
\end{equation*}

\begin{equation*}
J_{4D}(x_{0:n}) =  \frac{1}{2} \sum_{i=1}^n \|y_i - H(x_i) \|_R^2 + \frac{1}{2} \|x_0 - x^b \|_B^2
\end{equation*}
\begin{equation*}
J_{4D}(x_{0:n}) =  \frac{1}{2} \sum_{i=1}^n \|y_i - H(x_i) \|_R^2 + \frac{1}{2}\sum_{i=1}^{n} \|x_i - M(x_{i-1})\|^2_{Q}+ \frac{1}{2} \|x_0 - x^b \|_B^2
\end{equation*}


\begin{equation*}
p(y \mid x)
\end{equation*}

\begin{equation*}
\underbrace{x^a}_{\text{analysis}} = \underbrace{x^f}_{\text{forecast}} + \underbrace{K^*}_{\text{Kalman Gain}}\underbrace{\left(y - H(x^f)\right)}_{\text{Innovation vector}}
\end{equation*}

\begin{equation*}
\underbrace{P^a}_{\text{analysis error cov}} = (I - K^* H) \underbrace{P^f}_{\text{forecast error cov}}
\end{equation*}


\begin{equation*}
p(x) \approx \sum_{i=1}^N w_i \delta(x-x_i)
\end{equation*}
