:PROPERTIES:
:ID:       a1f7e01f-16c4-4382-84d3-7e2a6cea70e2
:END:
#+title: 2022-01-31
#+startup: latexpreview

* Variational formulation: notations and setting
 * $x \in \mathbb{X} \subseteq \mathbb{R}^n$ is the state vector
 * $y \in \mathbb{Y} \subseteq \mathbb{R}^p$ is the observation vector
 * $\mathcal{H}: \mathbb{X} \rightarrow \mathbb{Y}$ is the observation operator
 * $\mathcal{M}: \mathbb{X}\rightarrow \mathbb{X}$ is the model operator
 * $\|v\|^2_\Sigma = v^T \Sigma^{-1} v = \|\Sigma^{-1/2} v \|^2 = \langle\Sigma^{-1/2} v;\Sigma^{-1/2} v \rangle$

The cost function $J$ is defined as

\begin{align}
J(x) &= \frac{1}{2}\|(\mathcal{H}\circ \mathcal{M})(x) - y \|^2_{R} +\frac{1}{2} \|x - x_b \|^2_{B}\\
&= \frac{1}{2}\|G(x) - y \|^2_{R} + \frac{1}{2}\|x - x_b \|^2_{B}
\end{align}
with $G = \mathcal{H} \circ \mathcal{M}$


* Quasi-Newton methods for minimization
The minimization of $J$ is performed by finding a root of its gradient $\nabla J$.

The gradient of $J$ can be computed:
\begin{equation}
\nabla J(x) =
\nabla G(x)^TR^{-1}\left(G(x) - y\right) + B^{-1} (x - x_b)
\end{equation}

 and its Hessian is
 \begin{equation}
\nabla^2 J(x) = \left(\nabla G(x)^T R^{-1} \nabla G(x) + B^{-1}\right) + Q(x)
\end{equation}
where $Q(x)$ represents the higher-order terms
** Newton's method



Per, [[id:6d779bf7-10b4-46d0-b9d2-b4c1e0c328c8][Approximate GN methods for non-linear least-square problems]], we
Gauss-Newton method relies on the knowledge of the GN-Hessian

\begin{equation}
H_{\mathrm{GN}} = \nabla G(x)^T R^{-1} \nabla G(x) + B^{-1}
\end{equation}


