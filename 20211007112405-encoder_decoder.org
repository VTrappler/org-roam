:PROPERTIES:
:ID:       fdf7c607-fef1-41cd-902e-bcc74a404b67
:ROAM_ALIASES: "Latent Space" "Encoder Decoder"
:END:
#+title: Autoencoders
#+STARTUP: latexpreview
#+filetags: :DimensionReduction:MachineLearning:AutoEncoders:

An autoencoder is a type of [[id:7a245cfe-dcaa-47d6-a318-5574fab3b7ac][Neural Networks]] used for unsupervised
learning.  This relies on the training at the same time of an *encoder*,
which learns a representation of the data (akin to [[id:99cd54d1-bb93-4a2e-b6e2-ffb81fafa2e0][Dimension
Reduction]]) and of a *decoder*, which regenerates the data from the encoding.
https://en.wikipedia.org/wiki/Autoencoder

* Architecture
 * $\mathcal{X}$: Data space
 * $\mathcal{F}$: Feature space
 * $\phi: \mathcal{X} \rightarrow \mathcal{F}$: encoder
 * $\psi: \mathcal{F} \rightarrow \mathcal{X}$: decoder

The encoder and decoder are then defined as
\begin{align}
\left(\phi, \psi\right) &= \mathrm{argmin}_{\phi,\psi} \| \mathcal{X} - (\psi \circ \phi)\mathcal{X} \| \\
 &= \mathrm{argmin}_{\phi,\psi} \| (\mathcal{I}d - (\psi \circ \phi))\mathcal{X} \|
\end{align}


* Variational Autoencoders (VAE)
:PROPERTIES:
:ID:       fcf00225-0d0a-492a-a6f5-179fc401e1b3
:END:

This method is based on the construction of an autoencoder, which
minimizes the [[id:33a6b5ee-82e8-489a-858d-a634db231132][KL-divergence]] between the distribution of the input and
the output, in a [[id:f413aa4f-c6d9-497a-b02f-f0b4e5ff0c4e][Variational Bayes']] fashion

** Formulation
Given an input $x \sim P(x)$ where $P$ is unknown, and a latent
encoding vector $z$, the objective is to model the data as a
parametric distribution, $p_{\theta}$ with $\theta$ the set of network
parameters


* Latent space in practice

[[id:8bb3c55b-aa88-4763-bcec-e3e73227992a][Latent Space Data Assimilation using Deep Learning]] for instance.


