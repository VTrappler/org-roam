:PROPERTIES:
:ID:       9ff95f2d-88c7-4d67-a72a-5248f65235e6
:ROAM_REFS: cite:zahm_certified_2018
:END:
#+title: Certified Dimension Reduction
#+STARTUP: latexpreview
#+filetags: :DimensionReduction:

Certified Dimension Reduction is a [[id:99cd54d1-bb93-4a2e-b6e2-ffb81fafa2e0][Dimension Reduction]] methods,
introduced in cite:zahm_certified_2018, which relies on the
approximation of the likelihood by a *ridge function* (involving a
low-rank projector), in the sense of the [[id:33a6b5ee-82e8-489a-858d-a634db231132][KL-divergence]]

* Principle
** Bayesian Inverse Problem
Let $\mu$ be a probability distribution over $\mathcal{B}(\mathbb{R}^d)$
Given a measurable positive function $f$ with $\int f \mathrm{d}\, \mu < + \infty$
and let $\nu$ the the probability distribution such that
\begin{equation}
\frac{\mathrm{d} \nu}{\mathrm{d}\mu} \propto f
\end{equation}
in the sense of [[id:721678f4-dccf-41ed-b678-d111f8903007][Radon-Nikodym derivative]]. In a [[id:8dcedd6a-85dc-4af5-afde-5936cef961d6][Bayesian Inference]] paradigm, it
translates to:
 * $\mu$: prior
 * $\nu$: posterior
 * $f$: proportional to the likelihood function

** Ridge approximation

We wish to approximate the posterior $\nu$ by $\nu_r$ such that
\begin{equation}
\frac{\mathrm{d} \nu_r}{\mathrm{d} \mu} \propto g \circ P_r
\end{equation}
 * $P_r: \mathbb{R}^d \rightarrow \mathbb{R}^d$ is a rank-$r$ projector
 * $g: \mathbb{R}^d \rightarrow \mathbb{R}^+$ is a Borel function
   called the *profile function*

   Given a projector $P_r$,

\begin{equation}
x = x_r + x_{\bot} \quad \text{ with } \quad \left\{
  \begin{array}{ll}
    x_r &= P_rx \\
    x_{\bot} &= (I - P_r)x
  \end{array}
\right.
\end{equation}

If $r\ll d$, the approximation $\nu_r$ of $\nu$ consists in replacing
the high-dim likelihood $f$ by $x\mapsto g(P_rx) = g(x_r)$ which
depends only on $x_r\in\mathrm{Im}(P_r)\subset \mathbb{R}^r$, and is
constant on $\mathrm{Ker}(P_r) \in \mathbb{R}^{d-r}$.  However $\nu_r$
is not a low-dimensional distribution, since its support can be the
same as that of $\nu$.

The quality of the approximation is measured using the [[id:33a6b5ee-82e8-489a-858d-a634db231132][KL-divergence]]:
\begin{equation}
D_{\mathrm{KL}}(\nu \| \nu_r) = \int \log \left(\frac{\mathrm{d} \nu}{\mathrm{d} \nu_r}\right) \mathrm{d}\, \nu
\end{equation}
which exists if $\nu_r \ll \nu$ ($\nu_r$ absolutely continuous with
respect to $\nu$).

* Optimal profile
** TODO Conditional Expectation
We assume that we have $P_r$ a projector.

Let $\sigma(P_r)$ be the $\sigma$-algebra generated by $P_r$.
\begin{equation}
\sigma(P_r) = \left\{P_r^{-1}B \mid B \in \mathcal{B}(\mathbb{R}^d)\right\}
\end{equation}

$\mathbb{E}_{\mu}\left[f \mid \sigma(P_r)\right]$ is the conditional
expectation of $f$ given $\sigma(P_r)$, under the distribution $\mu$ (the prior).
By definition, it is the unique $\sigma(P_r)$ measurable function that satisfies
\begin{equation}
\int\mathbb{E}_{\mu}\left[f \mid \sigma(P_r)\right] h \, \mathrm{d}\mu = \int fh \, \mathrm{d}
\mu
\end{equation}
for any $\sigma(P_r)$ measurable function $h$

Finally, we consider $\nu_r^*$, such that
\begin{equation}
\frac{\mathrm{d}\nu_r^*}{\mathrm{d}\mu} \propto \mathbb{E}_{\mu}\left[f \mid \sigma(P_r)\right]
\end{equation}
 and we can verify that any $g^*$ which satisfy $g^* \circ
 P_r=\mathbb{E}_{\mu}\left[f \mid \sigma(P_r)\right]$ minimizes $g
 \mapsto D_{\mathrm{KL}}(\nu \| \nu_r)$
** Explicit expression of the conditional expectation

Let $\mu$ be a probability distribution on $\mathbb{R}^d$, with
Lebesgue's density $\rho$, and $P_r$ a rank $r$ projector, and
$U_{\bot} \in \mathbb{R}^{d \times (d-r)}$ is a matrix whose columns are a basis of $\mathhrm{Ker}(P_r)$.

We define the conditional probability density $p_{\bot}(\cdot \mid
P_rx)$ on $\mathbb{R}^{d-r}$ such that

\begin{equation}
p_{\bot}(\xi_{\bot} \mid P_rx)=  \frac{\rho(P_rx + U_{\bot}\xi_{\bot})}{\int_{\mathbb{R}^{d-r}}\rho(P_rx + U_{\bot}\xi^\prime) \, \mathrm{d}\xi^\prime} \propto {\rho(P_rx + U_{\bot}\xi_{\bot})} 
\end{equation}
i.e. the density of the "unprojected variable" once the projection is known.

Then conditional expectation can then be written as

\begin{equation}
\mathbb{E}_{\mu}\left[f \mid \sigma(P_r)\right]: x \mapsto \int_{\mathbb{R}^{d-r}} f(P_rx + U_{\bot}\xi_{\bot}) p_{\bot}(\xi_{\bot} \mid P_rx)\n\mathrm{d}\xi_{\bot}
\end{equation}
i.e. we average the measurable function $f$ with respect to the
"nuisance" (ie $Ker(P_r)$) parameters.

Once property is that the *conditional expectation is invariant with
respect to the image of the projector*. Only the Kernel of the projection is relevant.

Reducing the dimension in a Bayesian inverse problem consists in identifying $\mathrm{Ker}(P_r)$.
