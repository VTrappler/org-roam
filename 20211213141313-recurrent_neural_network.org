:PROPERTIES:
:ID:       f73cda73-3c82-43f4-9636-b2e409682afd
:ROAM_ALIASES: RNN
:END:
#+title: Recurrent Neural Network
#+filetags: :NeuralNetworks:MachineLearning:
#+startup: latexpreview

RNN are a class of [[id:7a245cfe-dcaa-47d6-a318-5574fab3b7ac][Neural Networks]], where connections between nodes
form a graph along a temporal sequence. Those NN show recurrent
connections. They are particularly useful for time series.


* Elman and Jordan Networks
Let us consider a 3 layered Neural Network:
 * $x$ is the input layer
 * $h$ the hidden layer
 * $y$ the output layer

   The hidden layer comprises classical neurons, but also a set of
   "context unit", which record the state of the hidden layer.

** Elman Network
Given the input layer at time $t$: $x_t$,
\begin{align}
h_t &= \sigma_h\left(w_h^T x_t + u_h^T h_{t-1} + b_h\right) \\
y_t &= \sigma_y\left(w_y^T h_t + b_y\right) \\
\end{align}

So $w$ are the weight matrices, $b$ the biases

** Jordan Networks
Similarly as for the Elman network, the hidden layer comprises the context unit, but the retroaction is on the output layer:
\begin{align}
h_t &= \sigma_h\left(w_h^T x_t + u_h^T y_{t-1} + b_h\right) \\
y_t &= \sigma_y\left(w_y^T h_t + b_y\right) \\
\end{align}

* Cons
 * Notoriously hard to train due to the vanishing gradient problems


 
* RNN and ODE
  Let us consider an Elman Network:
  \begin{equation}
h_t = \sigma(Ux_t + Wh_{t-1} + b) = f(h_{t-1}, \theta)
\end{equation}
where $\theta =[W, U, x]$ is the parameter vector.

Let us consider the 1st order ODE
\begin{equation}
h^'(t) = F(h(t-1))
\end{equation}
The forward euler scheme gives
\begin{equation}
h_t = h_{t-1} + \epsilon F(h_{t-1})  \quad h_{t_0} = h(0)
\end{equation}
